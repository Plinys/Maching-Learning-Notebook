# 决策树
决策树的树状结构中每一个非叶节点表示一个**特征属性**，每一个分支代表这个特征属性在某个值域上的输出，  
每一个叶节点存放一个**类别**  

决策过程从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，  
直到到达叶子节点，并将叶子节点存放的类别作为决策结果 

**重点**：如何选择作为节点的特征属性  
划分数据集的大原则：**将无序的数据变得更加有序**

## 信息熵
[参考文章][https://zhuanlan.zhihu.com/p/26486223]  

$$H(x)=-\sum_{i=1}^n P(x_i) log_2 P(x_i)$$
  
信息量度量一个具体事件发生所带来的信息，而熵则是在结果出来之前对可能产生信息量的期望  
**理解**  
信息熵还可以作为一个系统复炸程度的度量，如果系统越复杂，信息熵越大  
通俗来说，一个事件发生的概率越大，则信息量熵越小。
### Python 实现计算信息熵
```
def calcShannonEnt(dataSet):
    numEntries = len(dataSet)
    labelCounts = {}
    for featVec in dataSet:
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.key():
            labelCounts[currentLabel] = 0
            labelCounts[currentLabel] += 1
        shannonEnt = 0.0
        for key in labelCounts:
        prob = float(labelCounts[key])/numEntries
        shannonEnt -= prob * math.log(prob,2)
     return shannonEnt
```

## 条件熵
[参考文章][https://zhuanlan.zhihu.com/p/26551798]  

条件熵 为在X条件下，Y的条件概率分布的熵对与X的数学期望，  
也可理解为在已知随机变量X的条件下随机变量Y的不确定性。

$$
\begin{aligned}
H(Y|X)&=\sum_{x\in X} P(x) H(Y|X=x)\\\ 
&=-\sum_{x\in X} P(x) \sum_{y\in Y} P(Y|X) log_2 P(y|x)\\\ 
&=-\sum_{x\in X} \sum_{y\in Y} P(x,y) log_2 P(y|x)
\end{aligned}
$$

## 信息增益
**信息增益 = 信息熵 - 条件熵**
表示得知a属性的信息而使得集合的不确定度减少的程度

## ID3算法
以信息论为基础，以信息熵和信息增益为衡量标准  
1. 解决如何选择特征作为划分数据集的标准  
  选择**信息增益最大**的属性作为特征的分类
2. 解决如何判断划分的结束  
  (1). 划分出的类属于同一类
  (2). 无属性可供再分

